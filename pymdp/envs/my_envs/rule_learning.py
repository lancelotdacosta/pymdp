#!/usr/bin/env python
# -*- coding: utf-8 -*-

""" Rule Learning Environment

This environment implements a rule learning task where an agent must learn
and follow certain rules to achieve goals.

The environment consists of three outcome modalities:
1. Colored visual cues (what) - providing visual information
2. Proprioceptive cues (where) - signaling the direction of gaze
3. Auditory cues (feedback) - providing feedback on actions

These three types of outcomes are generated by interactions among four hidden states/factors:
1. Rule - an abstract rule indicating the location of an informative color cue
2. Colour - the correct color
3. Where - where attention or saccadic eye movements are directed
4. Choice - the (manual) response
"""

from pymdp.envs import Env
from pymdp import utils, maths
import numpy as np

# Hidden state factor indices
RULE_FACTOR_ID = 0    # rule state factor (3 states)
COLOUR_FACTOR_ID = 1  # colour state factor (3 states) correct colour
WHERE_FACTOR_ID = 2   # location state factor (4 states)
CHOICE_FACTOR_ID = 3  # choice state factor (4 states)

# Location state/observation labels
LEFT = 0
TOP = 1
RIGHT = 2
CENTRE = 3

# Rule state labels
RULE_LEFT = 0    # Left rule
RULE_TOP = 1     # Top rule
RULE_RIGHT = 2   # Right rule

# Choice state labels
CHOICE_RED = 0
CHOICE_GREEN = 1
CHOICE_BLUE = 2
CHOICE_UNDECIDED = 3

# What observation labels
WHAT_RED = 0
WHAT_GREEN = 1
WHAT_BLUE = 2
WHAT_WHITE = 3

# Observation modality indices
WHAT_OBS_ID = 0    # What modality (4 states - red, green, blue, white)
WHERE_OBS_ID = 1   # Where modality (4 states - left:0, top:1, right:2, centre:3)
FEEDBACK_OBS_ID = 2  # Feedback modality (3 states - neutral/correct/incorrect)

class RuleLearningEnv(Env):
    """
    A rule learning environment where an agent must discover and follow rules
    to achieve goals.

    Hidden State Factors:
    - rule: The current rule (3 states: left, top, right)
        This rule can be inferred by looking at the top location:
        - Left rule -> Red at top
        - Top rule -> Green at top
        - Right rule -> Blue at top
    - colour: The correct color state (3 states: red, green, blue)
        A choice is correct if it matches this hidden color
    - where: The location state (4 states: left, top, right, center)
        Each location shows a specific color:
        - Center always shows white
        - Top shows color determined by rule
        - Left and right show random fixed colors
    - choice: The choice state (4 states: red, green, blue, undecided)

    Observation Modalities:
    - what: Color being observed (4 states: red, green, blue, white)
        Depends on location and rule (for top location)
    - where: Location being observed (4 states)
        Pure deterministic mapping from where state
    - feedback: Feedback on choices (3 states: neutral, correct, incorrect)
        Deterministic based on whether choice matches hidden color
    """
    def __init__(self):
        """
        Initialize the rule learning environment with:
        
        Hidden State Factors:
        - rule: The current rule in effect (3 states: left, top, right)
        - colour: The color state (3 states)
        - where: The location state (4 states)
        - choice: The choice state (4 states: red, green, blue, undecided)
        
        Observation Modalities:
        - what: What is being observed (4 states: red, green, blue, white)
        - where: Where it is being observed (4 states)
        - feedback: Feedback on choices (3 states)
        """
        super(RuleLearningEnv, self).__init__()
        
        # Define the number of states for each factor
        self.num_rules = 3    # three different rules
        self.num_colours = 3  # three different colors
        self.num_locations = 4  # four different locations
        self.num_choices = 4  # four different choices
        
        # Define the state dimensionalities
        self.num_states = [
            self.num_rules,
            self.num_colours,
            self.num_locations,
            self.num_choices
        ]
        
        # Define number of controls for each factor
        self.num_controls = [
            1,                  # rule (not controllable)
            1,                  # colour (not controllable)
            self.num_locations, # where (perfectly controllable)
            self.num_choices    # choice (perfectly controllable)
        ]
        
        # Define number of observations for each modality
        self.num_obs = [
            4,  # what observation (4 states)
            4,  # where observation (4 states)
            3   # feedback observation (3 states)
        ]
        
        self.num_factors = len(self.num_states)
        self.num_modalities = len(self.num_obs)
        
        # Initialize distributions
        self._transition_dist = self._construct_transition_dist()
        self._likelihood_dist = self._construct_likelihood_dist()
        
        # Initialize state
        self._state = None
        self._previous_choice = None  # For tracking choice history
    
    def reset(self, state=None):
        """Reset the environment to initial state.
        
        If no state is provided, initializes with:
        - Random rule state (left, top, or right)
        - Random correct color state (red, green, or blue)
        - Center location (index 3)
        - Undecided choice (index 3)
        
        Args:
            state: Optional pre-specified state. If provided, the environment
                  will be initialized to this state instead of random.
        
        Returns:
            list: Initial observations from [what, where, feedback] modalities
        """
        if state is None:
            rule_state = utils.onehot(np.random.randint(self.num_rules), self.num_rules)
            colour_state = utils.onehot(np.random.randint(self.num_colours), self.num_colours)
            where_state = utils.onehot(3, self.num_locations)  # Always start at centre (index 3)
            choice_state = utils.onehot(3, self.num_choices)  # Always start with undecided
            
            full_state = utils.obj_array(self.num_factors)
            full_state[RULE_FACTOR_ID] = rule_state
            full_state[COLOUR_FACTOR_ID] = colour_state
            full_state[WHERE_FACTOR_ID] = where_state
            full_state[CHOICE_FACTOR_ID] = choice_state
            
            self._state = full_state
        else:
            self._state = state
        return self._get_observation()

    def step(self, actions):
        """Take a step in the environment given actions."""
        prob_states = utils.obj_array(self.num_factors)
        for factor, state in enumerate(self._state):
            prob_states[factor] = self._transition_dist[factor][:, :, int(actions[factor])].dot(state)
        state = [utils.sample(ps_i) for ps_i in prob_states]
        self._state = self._construct_state(state)
        return self._get_observation()

    def _get_observation(self):
        """Generate observations from current state."""
        prob_obs = [maths.spm_dot(A_m, self._state) for A_m in self._likelihood_dist]
        obs = [utils.sample(po_i) for po_i in prob_obs]
        return obs

    def _construct_transition_dist(self):
        """
        Construct the transition distributions for all state factors.
        
        First two factors (rule, colour) are not controllable.
        Last two factors (where, choice) are perfectly controllable.
        """
        B = utils.obj_array(self.num_factors)
        
        # Rule transitions (static - not controllable)
        B[RULE_FACTOR_ID] = np.eye(self.num_rules).reshape(self.num_rules, self.num_rules, 1)
        
        # Colour transitions (static - not controllable)
        B[COLOUR_FACTOR_ID] = np.eye(self.num_colours).reshape(self.num_colours, self.num_colours, 1)
        
        # Where transitions (perfectly controllable)
        B_where = np.eye(self.num_locations)
        B_where = B_where.reshape(self.num_locations, self.num_locations, 1)
        B_where = np.tile(B_where, (1, 1, self.num_locations))
        B[WHERE_FACTOR_ID] = B_where.transpose(1, 2, 0)
        
        # Choice transitions (perfectly controllable)
        B_choice = np.eye(self.num_choices)
        B_choice = B_choice.reshape(self.num_choices, self.num_choices, 1)
        B_choice = np.tile(B_choice, (1, 1, self.num_choices))
        B[CHOICE_FACTOR_ID] = B_choice.transpose(1, 2, 0)
        
        return B

    def _construct_likelihood_dist(self):
        """Construct the likelihood distribution mapping hidden states to observations."""
        A = utils.obj_array_zeros([[obs_dim] + self.num_states for obs_dim in self.num_obs])
        
        # Randomly assign colors to left and right locations (indices 0,2)
        # Colors are 0,1,2 (excluding 3 which is white)
        location_colors = np.random.choice(3, size=2, replace=True)  # Random colors for left and right
        
        # Set up deterministic where observation mapping (independent of other factors)
        for where in range(self.num_locations):
            A[WHERE_OBS_ID][where, :, :, where, :] = 1.0  # Deterministic mapping from where state to where observation
        
        # Iterate through all possible state combinations for other observations
        for rule in range(self.num_rules):
            for colour in range(self.num_colours):
                for where in range(self.num_locations):
                    for choice in range(self.num_choices):
                        # What observation depends on location and rule
                        if where == 3:  # Center location
                            A[WHAT_OBS_ID][3, rule, colour, where, choice] = 1.0  # Always white
                        elif where == 1:  # Colour Top location <- Rule 
                            if rule == 0:  # Left rule -> red
                                A[WHAT_OBS_ID][0, rule, colour, where, choice] = 1.0
                            elif rule == 1:  # Top rule -> green
                                A[WHAT_OBS_ID][1, rule, colour, where, choice] = 1.0
                            else:  # Right rule -> blue
                                A[WHAT_OBS_ID][2, rule, colour, where, choice] = 1.0
                        elif where == 0:  # Left location
                            A[WHAT_OBS_ID][location_colors[0], rule, colour, where, choice] = 1.0
                        else:  # Right location
                            A[WHAT_OBS_ID][location_colors[1], rule, colour, where, choice] = 1.0
                        
                        # Feedback observation depends on rule and choice
                        if choice != 3:  # If a choice has been made
                            correct_choice = self._evaluate_choice(rule, colour, where, choice)
                            if correct_choice:
                                A[FEEDBACK_OBS_ID][1, rule, colour, where, choice] = 1.0  # Correct (deterministic)
                            else:
                                A[FEEDBACK_OBS_ID][2, rule, colour, where, choice] = 1.0  # Incorrect (deterministic)
                        else:  # No choice made yet
                            A[FEEDBACK_OBS_ID][0, rule, colour, where, choice] = 1.0  # Neutral feedback
        
        return A

    def _evaluate_choice(self, rule, colour, where, choice):
        """
        Evaluate if the choice is correct given the current state.
        
        Colors and Choices:
        0: Red
        1: Green
        2: Blue
        3: Undecided
        
        A choice is correct if and only if it matches the hidden color state.
        """
        if choice == 3:  # No choice made
            return False
            
        # Choice is correct if it matches the hidden color state
        return choice == colour

    def _construct_state(self, state_tuple):
        """Construct a proper state array from state indices."""
        state = utils.obj_array(self.num_factors)
        for f, ns in enumerate(self.num_states):
            state[f] = utils.onehot(state_tuple[f], ns)
        return state

    def get_likelihood_dist(self):
        """Return the likelihood distribution of the environment."""
        return self._likelihood_dist

    def get_transition_dist(self):
        """Return the transition distribution of the environment."""
        return self._transition_dist

    def get_state(self):
        """Return the current state of the environment.

        Returns:
            list: Current state array containing the state for each factor
                 [rule_state, colour_state, where_state, choice_state]
        """
        return self._state

    @property
    def state(self):
        return self._state
